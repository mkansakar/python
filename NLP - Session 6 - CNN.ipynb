{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13758,
     "status": "ok",
     "timestamp": 1606653186643,
     "user": {
      "displayName": "Raghav Nyapati",
      "photoUrl": "",
      "userId": "13728981372697321629"
     },
     "user_tz": -330
    },
    "id": "3Zyu-D1l8XUS",
    "outputId": "dfbe6310-ede3-4868-8f55-2c201222e0c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pugnlp\n",
      "  Downloading pugnlp-0.2.6-py2.py3-none-any.whl (706 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from pugnlp) (1.5.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from pugnlp) (4.55.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from pugnlp) (0.24.0)\n",
      "Requirement already satisfied: jupyter in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from pugnlp) (1.0.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from pugnlp) (0.36.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from pugnlp) (3.3.3)\n",
      "Requirement already satisfied: pip in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from pugnlp) (20.3.3)\n",
      "Collecting coverage\n",
      "  Downloading coverage-5.3.1-cp37-cp37m-win_amd64.whl (212 kB)\n",
      "Collecting future\n",
      "  Using cached future-0.18.2-py3-none-any.whl\n",
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from gensim->pugnlp) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from gensim->pugnlp) (1.19.2)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.0.1.tar.gz (117 kB)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from jupyter->pugnlp) (5.6.1)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from jupyter->pugnlp) (4.7.7)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from jupyter->pugnlp) (5.4.2)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from jupyter->pugnlp) (7.5.1)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from jupyter->pugnlp) (6.2.0)\n",
      "Requirement already satisfied: notebook in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from jupyter->pugnlp) (6.1.5)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from ipykernel->jupyter->pugnlp) (5.0.5)\n",
      "Requirement already satisfied: ipython>=5.0.0 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from ipykernel->jupyter->pugnlp) (7.19.0)\n",
      "Requirement already satisfied: tornado>=4.2 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from ipykernel->jupyter->pugnlp) (6.1)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from ipykernel->jupyter->pugnlp) (6.1.7)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->pugnlp) (0.18.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->pugnlp) (2.7.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->pugnlp) (51.0.0.post20201207)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->pugnlp) (3.0.8)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->pugnlp) (0.7.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->pugnlp) (0.4.4)\n",
      "Requirement already satisfied: backcall in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->pugnlp) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from ipython>=5.0.0->ipykernel->jupyter->pugnlp) (4.4.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter->pugnlp) (0.8.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->pugnlp) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from traitlets>=4.1.0->ipykernel->jupyter->pugnlp) (0.2.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from ipywidgets->jupyter->pugnlp) (5.0.8)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from ipywidgets->jupyter->pugnlp) (3.5.1)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (4.7.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (20.3.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (2.0.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (0.17.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from notebook->jupyter->pugnlp) (0.9.1)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from notebook->jupyter->pugnlp) (20.0.0)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from notebook->jupyter->pugnlp) (1.5.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from notebook->jupyter->pugnlp) (0.9.0)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from notebook->jupyter->pugnlp) (20.1.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from notebook->jupyter->pugnlp) (2.11.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from jupyter-client->ipykernel->jupyter->pugnlp) (2.8.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (300)\n",
      "Requirement already satisfied: pywinpty>=0.5 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from terminado>=0.8.3->notebook->jupyter->pugnlp) (0.5.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from argon2-cffi->notebook->jupyter->pugnlp) (1.14.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter->pugnlp) (2.20)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from importlib-metadata->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->pugnlp) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from jinja2->notebook->jupyter->pugnlp) (1.1.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from matplotlib->pugnlp) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from matplotlib->pugnlp) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from matplotlib->pugnlp) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from matplotlib->pugnlp) (8.0.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from nbconvert->jupyter->pugnlp) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from nbconvert->jupyter->pugnlp) (0.3)\n",
      "Requirement already satisfied: bleach in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from nbconvert->jupyter->pugnlp) (3.2.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from nbconvert->jupyter->pugnlp) (1.4.3)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from nbconvert->jupyter->pugnlp) (0.6.0)\n",
      "Requirement already satisfied: testpath in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from nbconvert->jupyter->pugnlp) (0.4.4)\n",
      "Requirement already satisfied: webencodings in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from bleach->nbconvert->jupyter->pugnlp) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from bleach->nbconvert->jupyter->pugnlp) (20.8)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "Requirement already satisfied: click in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from nltk->pugnlp) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from nltk->pugnlp) (1.0.0)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp37-cp37m-win_amd64.whl (8.7 MB)\n",
      "Collecting pytz>=2017.2\n",
      "  Using cached pytz-2020.5-py2.py3-none-any.whl (510 kB)\n",
      "Collecting plotly\n",
      "  Downloading plotly-4.14.1-py2.py3-none-any.whl (13.2 MB)\n",
      "Collecting retrying>=1.3.3\n",
      "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
      "Collecting pypandoc\n",
      "  Downloading pypandoc-1.5.tar.gz (26 kB)\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python-Levenshtein-0.12.0.tar.gz (48 kB)\n",
      "Collecting python-slugify\n",
      "  Using cached python_slugify-4.0.1-py2.py3-none-any.whl\n",
      "Collecting text-unidecode>=1.3\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: qtpy in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from qtconsole->jupyter->pugnlp) (1.9.0)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.11.13-cp37-cp37m-win_amd64.whl (269 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mahesh\\anaconda3\\envs\\home\\lib\\site-packages (from scikit-learn->pugnlp) (2.1.0)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.11.1-py3-none-any.whl (285 kB)\n",
      "Building wheels for collected packages: smart-open, nltk, retrying, pypandoc, python-Levenshtein\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-4.0.1-py3-none-any.whl size=108248 sha256=97d80926fb40716004cf716ffd654897d85a7bb76dc1c3304091f683eeb80b13\n",
      "  Stored in directory: c:\\users\\mahesh\\appdata\\local\\pip\\cache\\wheels\\34\\3d\\14\\f19c01a19c9201cdb6a76b049904d5226912569be919ad1eae\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434675 sha256=daee7d720109b81c698434d70847cdb4b850974aa4153b4865bad68d6646d088\n",
      "  Stored in directory: c:\\users\\mahesh\\appdata\\local\\pip\\cache\\wheels\\45\\6c\\46\\a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "  Building wheel for retrying (setup.py): started\n",
      "  Building wheel for retrying (setup.py): finished with status 'done'\n",
      "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11429 sha256=7bb10559c3b09f3649b4d5581571a9444169b307ec257c9b1174604dd457e9fa\n",
      "  Stored in directory: c:\\users\\mahesh\\appdata\\local\\pip\\cache\\wheels\\f9\\8d\\8d\\f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
      "  Building wheel for pypandoc (setup.py): started\n",
      "  Building wheel for pypandoc (setup.py): finished with status 'done'\n",
      "  Created wheel for pypandoc: filename=pypandoc-1.5-py3-none-any.whl size=17037 sha256=dd1ff28c03b244e99c7ad24e16df595bfb235b96f78015b038b4156a7ba83e1c\n",
      "  Stored in directory: c:\\users\\mahesh\\appdata\\local\\pip\\cache\\wheels\\06\\a7\\1d\\3259fbf0089b15d491d9166b97de3a23d1f915bf7591abafc7\n",
      "  Building wheel for python-Levenshtein (setup.py): started\n",
      "  Building wheel for python-Levenshtein (setup.py): finished with status 'done'\n",
      "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp37-cp37m-win_amd64.whl size=81477 sha256=72e9d2b80ab69293fb8ef586d3193e05644ab753f2f5346807061cbd988bfdd1\n",
      "  Stored in directory: c:\\users\\mahesh\\appdata\\local\\pip\\cache\\wheels\\f0\\9b\\13\\49c281164c37be18343230d3cd0fca29efb23a493351db0009\n",
      "Successfully built smart-open nltk retrying pypandoc python-Levenshtein\n",
      "Installing collected packages: pytz, text-unidecode, smart-open, retrying, regex, pandas, Cython, seaborn, python-slugify, python-Levenshtein, pypandoc, plotly, nltk, gensim, fuzzywuzzy, future, coverage, pugnlp\n",
      "Successfully installed Cython-0.29.14 coverage-5.3.1 future-0.18.2 fuzzywuzzy-0.18.0 gensim-3.8.3 nltk-3.5 pandas-1.1.5 plotly-4.14.1 pugnlp-0.2.6 pypandoc-1.5 python-Levenshtein-0.12.0 python-slugify-4.0.1 pytz-2020.5 regex-2020.11.13 retrying-1.3.3 seaborn-0.11.1 smart-open-4.0.1 text-unidecode-1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahesh\\Anaconda3\\envs\\home\\lib\\site-packages\\pugnlp\\constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "C:\\Users\\Mahesh\\Anaconda3\\envs\\home\\lib\\site-packages\\pugnlp\\constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import tqdm\n",
    "\n",
    "import requests\n",
    "! pip install pugnlp\n",
    "from pugnlp.futil import path_status, find_files\n",
    "import numpy as np  # Keras takes care of most of this but it likes to see Numpy arrays\n",
    "from keras.preprocessing import sequence    # A helper module to handle padding input\n",
    "from keras.models import Sequential         # The base keras Neural Network model\n",
    "from keras.layers import Dense, Dropout, Activation   # The layer objects we will pile into the model\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1527,
     "status": "ok",
     "timestamp": 1606655471233,
     "user": {
      "displayName": "Raghav Nyapati",
      "photoUrl": "",
      "userId": "13728981372697321629"
     },
     "user_tz": -330
    },
    "id": "w-LwlCq48XUS"
   },
   "outputs": [],
   "source": [
    "# From the nlpia package for downloading data too big for the repo\n",
    "\n",
    "BIG_URLS = {\n",
    "    'w2v': (\n",
    "        'https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1',\n",
    "        1647046227,\n",
    "    ),\n",
    "    'slang': (\n",
    "        'https://www.dropbox.com/s/43c22018fbfzypd/slang.csv.gz?dl=1',\n",
    "        117633024,\n",
    "    ),\n",
    "    'tweets': (\n",
    "        'https://www.dropbox.com/s/5gpb43c494mc8p0/tweets.csv.gz?dl=1',\n",
    "        311725313,\n",
    "    ),\n",
    "    'lsa_tweets': (\n",
    "        'https://www.dropbox.com/s/rpjt0d060t4n1mr/lsa_tweets_5589798_2003588x200.tar.gz?dl=1',\n",
    "        3112841563,  # 3112841312,\n",
    "    ),\n",
    "    'imdb': (\n",
    "        'https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz?dl=1',\n",
    "        3112841563,  # 3112841312,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1725,
     "status": "ok",
     "timestamp": 1606655455011,
     "user": {
      "displayName": "Raghav Nyapati",
      "photoUrl": "",
      "userId": "13728981372697321629"
     },
     "user_tz": -330
    },
    "id": "S86Eukrs8XUS"
   },
   "outputs": [],
   "source": [
    "# These functions are part of the nlpia package which can be pip installed and run from there.\n",
    "def dropbox_basename(url):\n",
    "    filename = os.path.basename(url)\n",
    "    match = re.findall(r'\\?dl=[0-9]$', filename)\n",
    "    if match:\n",
    "        return filename[:-len(match[0])]\n",
    "    return filename\n",
    "\n",
    "def download_file(url, data_path='.', filename=None, size=None, chunk_size=4096, verbose=True):\n",
    "    \"\"\"Uses stream=True and a reasonable chunk size to be able to download large (GB) files over https\"\"\"\n",
    "    if filename is None:\n",
    "        filename = dropbox_basename(url)\n",
    "    file_path = os.path.join(data_path, filename)\n",
    "    if url.endswith('?dl=0'):\n",
    "        url = url[:-1] + '1'  # noninteractive download\n",
    "    if verbose:\n",
    "        tqdm_prog = tqdm\n",
    "        print('requesting URL: {}'.format(url))\n",
    "    else:\n",
    "        tqdm_prog = no_tqdm\n",
    "    r = requests.get(url, stream=True, allow_redirects=True)\n",
    "    size = r.headers.get('Content-Length', None) if size is None else size\n",
    "    print('remote size: {}'.format(size))\n",
    "\n",
    "    stat = path_status(file_path)\n",
    "    print('local size: {}'.format(stat.get('size', None)))\n",
    "    if stat['type'] == 'file' and stat['size'] == size:  # TODO: check md5 or get the right size of remote file\n",
    "        r.close()\n",
    "        return file_path\n",
    "\n",
    "    print('Downloading to {}'.format(file_path))\n",
    "\n",
    "    with open(file_path, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            if chunk:  # filter out keep-alive chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "    r.close()\n",
    "    return file_path\n",
    "\n",
    "def untar(fname):\n",
    "    if fname.endswith(\"tar.gz\"):\n",
    "        with tarfile.open(fname) as tf:\n",
    "            tf.extractall()\n",
    "    else:\n",
    "        print(\"Not a tar.gz file: {}\".format(fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "executionInfo": {
     "elapsed": 1541,
     "status": "error",
     "timestamp": 1606655460171,
     "user": {
      "displayName": "Raghav Nyapati",
      "photoUrl": "",
      "userId": "13728981372697321629"
     },
     "user_tz": -330
    },
    "id": "osbEjVhr8XUS",
    "outputId": "b007ab12-29d5-4097-f6f4-b536f94158e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting URL: https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1\n",
      "remote size: 1647046227\n",
      "local size: None\n",
      "Downloading to .\\GoogleNews-vectors-negative300.bin.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.\\\\GoogleNews-vectors-negative300.bin.gz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_file(BIG_URLS['w2v'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32635,
     "status": "ok",
     "timestamp": 1606653505992,
     "user": {
      "displayName": "Raghav Nyapati",
      "photoUrl": "",
      "userId": "13728981372697321629"
     },
     "user_tz": -330
    },
    "id": "kHLk53IR8XUS",
    "outputId": "75e335ca-9058-4793-e321-c0b772143386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting URL: https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz?dl=1\n",
      "remote size: 84125825\n",
      "local size: None\n",
      "Downloading to .\\aclImdb_v1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "untar(download_file(BIG_URLS['imdb'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2131,
     "status": "ok",
     "timestamp": 1606653599664,
     "user": {
      "displayName": "Raghav Nyapati",
      "photoUrl": "",
      "userId": "13728981372697321629"
     },
     "user_tz": -330
    },
    "id": "bFF2xTnf8XUS",
    "outputId": "ec721f06-0ed9-44fa-9199-9cdea23b72dc"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-022febfaefea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_process_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data\\\\aclImdb\\\\train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "def pre_process_data(filepath):\n",
    "    \"\"\"\n",
    "    This is dependent on your training data source but we will try to generalize it as best as possible.\n",
    "    \"\"\"\n",
    "    positive_path = os.path.join(filepath, 'pos')\n",
    "    negative_path = os.path.join(filepath, 'neg')\n",
    "    \n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
    "        with open(filename, 'r') as f:\n",
    "            dataset.append((pos_label, f.read()))\n",
    "            \n",
    "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
    "        with open(filename, 'r') as f:\n",
    "            dataset.append((neg_label, f.read()))\n",
    "    \n",
    "    shuffle(dataset)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = pre_process_data('data\\\\aclImdb\\\\train')\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 10099,
     "status": "ok",
     "timestamp": 1606654404797,
     "user": {
      "displayName": "Raghav Nyapati",
      "photoUrl": "",
      "userId": "13728981372697321629"
     },
     "user_tz": -330
    },
    "id": "dvbnMuGV8XUT"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)\n",
    "\n",
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "\n",
    "            except KeyError:\n",
    "                pass  # No matching token in the Google w2v vocab\n",
    "            \n",
    "        vectorized_data.append(sample_vecs)\n",
    "\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1597,
     "status": "ok",
     "timestamp": 1606654554175,
     "user": {
      "displayName": "Raghav Nyapati",
      "photoUrl": "",
      "userId": "13728981372697321629"
     },
     "user_tz": -330
    },
    "id": "ALa5z8d68XUT"
   },
   "outputs": [],
   "source": [
    "def collect_expected(dataset):\n",
    "    \"\"\" Peel of the target values from the dataset \"\"\"\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "executionInfo": {
     "elapsed": 1696,
     "status": "error",
     "timestamp": 1606654560594,
     "user": {
      "displayName": "Raghav Nyapati",
      "photoUrl": "",
      "userId": "13728981372697321629"
     },
     "user_tz": -330
    },
    "id": "autOhrHw8XUT",
    "outputId": "68e883c7-c2c3-4ea9-d103-34ba95c9e47b"
   },
   "outputs": [],
   "source": [
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 2169,
     "status": "ok",
     "timestamp": 1606654191226,
     "user": {
      "displayName": "Raghav Nyapati",
      "photoUrl": "",
      "userId": "13728981372697321629"
     },
     "user_tz": -330
    },
    "id": "P0R5kmL38XUT"
   },
   "outputs": [],
   "source": [
    "split_point = int(len(vectorized_data)*.8)\n",
    "\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 1564,
     "status": "ok",
     "timestamp": 1606654263208,
     "user": {
      "displayName": "Raghav Nyapati",
      "photoUrl": "",
      "userId": "13728981372697321629"
     },
     "user_tz": -330
    },
    "id": "WAEjw8Vv8XUT"
   },
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "batch_size = 32         # How many samples to show the net before backpropogating the error and updating the weights\n",
    "embedding_dims = 300    # Length of the token vectors we will create for passing into the Convnet\n",
    "filters = 250           # Number of filters we will train\n",
    "kernel_size = 3         # The width of the filters, actual filters will each be a matrix of weights of size: embedding_dims x kernel_size or 50 x 3 in our case\n",
    "hidden_dims = 250       # Number of neurons in the plain feed forward net at the end of the chain\n",
    "epochs = 2              # Number of times we will pass the entire training dataset through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1588,
     "status": "ok",
     "timestamp": 1606654268779,
     "user": {
      "displayName": "Raghav Nyapati",
      "photoUrl": "",
      "userId": "13728981372697321629"
     },
     "user_tz": -330
    },
    "id": "0sSDGWT08XUT"
   },
   "outputs": [],
   "source": [
    "# Must manually pad/truncate\n",
    "\n",
    "def pad_trunc(data, maxlen):\n",
    "    \"\"\" For a given dataset pad with zero vectors or truncate to maxlen \"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    # Create a vector of 0's the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "\n",
    "    for sample in data:\n",
    " \n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "875Xu-7y8XUT"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6be14357e22c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_trunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_trunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-108037f76afd>\u001b[0m in \u001b[0;36mpad_trunc\u001b[1;34m(data, maxlen)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Create a vector of 0's the length of our word vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mzero_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mzero_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Fp9i6xZQ8XUT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fc43b823f4e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m           validation_data=(x_test, y_test))\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mmodel_structure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cnn_model.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\home\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1062\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1064\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\home\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1097\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m     self._adapter = adapter_cls(\n\u001b[0;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\home\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m   \u001b[1;34m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 958\u001b[1;33m   \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mALL_ADAPTER_CLS\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcan_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    959\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m     \u001b[1;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\home\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m   \u001b[1;34m\"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 958\u001b[1;33m   \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mALL_ADAPTER_CLS\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcan_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    959\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m     \u001b[1;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\home\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mcan_handle\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    610\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcan_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m     \u001b[0mhandles_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_list_of_scalars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m     \u001b[0mhandles_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\home\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_is_list_of_scalars\u001b[1;34m(inp)\u001b[0m\n\u001b[0;32m    621\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_list_of_scalars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1,\n",
    "                 input_shape=(maxlen, embedding_dims)))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "model_structure = model.to_json()\n",
    "with open(\"cnn_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"cnn_weights.h5\")\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YYhrIHkl8XUT"
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "with open(\"cnn_model.json\", \"r\") as json_file:\n",
    "    json_string = json_file.read()\n",
    "model = model_from_json(json_string)\n",
    "\n",
    "model.load_weights('cnn_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-03r38Ag8XUT"
   },
   "outputs": [],
   "source": [
    "sample_1 = \"I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSe1qsXM8XUT"
   },
   "outputs": [],
   "source": [
    "# We pass a dummy value in the first element of the tuple just because our helper expects it from the way processed the initial data.  That value won't ever see the network, so it can be whatever.\n",
    "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
    "\n",
    "# Tokenize returns a list of the data (length 1 here)\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "\n",
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
    "model.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6PFl5cy8XUT"
   },
   "outputs": [],
   "source": [
    "model.predict_classes(test_vec)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "home",
   "language": "python",
   "name": "home"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
