{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQHtaz9tim2n"
   },
   "source": [
    "# Build Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ITobZzVQim2p"
   },
   "outputs": [],
   "source": [
    "documents = ['Ram is in eighth grade and ready to go to ninth grade',\n",
    "            'Shanti is in sixth grade']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcLqGaQuim2r"
   },
   "source": [
    "# Build Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvMRGzkuim2r"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T1_e6uxlim2u"
   },
   "source": [
    "Initialize and fit the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PUo8A95yim2v"
   },
   "outputs": [],
   "source": [
    "t = tf.keras.preprocessing.text.Tokenizer(num_words=15) # num_words -> Vocablury size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rLthSPZFF8t5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchar_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moov_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdocument_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Text tokenization utility class.\n",
       "\n",
       "This class allows to vectorize a text corpus, by turning each\n",
       "text into either a sequence of integers (each integer being the index\n",
       "of a token in a dictionary) or into a vector where the coefficient\n",
       "for each token could be binary, based on word count, based on tf-idf...\n",
       "\n",
       "# Arguments\n",
       "    num_words: the maximum number of words to keep, based\n",
       "        on word frequency. Only the most common `num_words-1` words will\n",
       "        be kept.\n",
       "    filters: a string where each element is a character that will be\n",
       "        filtered from the texts. The default is all punctuation, plus\n",
       "        tabs and line breaks, minus the `'` character.\n",
       "    lower: boolean. Whether to convert the texts to lowercase.\n",
       "    split: str. Separator for word splitting.\n",
       "    char_level: if True, every character will be treated as a token.\n",
       "    oov_token: if given, it will be added to word_index and used to\n",
       "        replace out-of-vocabulary words during text_to_sequence calls\n",
       "\n",
       "By default, all punctuation is removed, turning the texts into\n",
       "space-separated sequences of words\n",
       "(words maybe include the `'` character). These sequences are then\n",
       "split into lists of tokens. They will then be indexed or vectorized.\n",
       "\n",
       "`0` is a reserved index that won't be assigned to any word.\n",
       "\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.7/site-packages/keras_preprocessing/text.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?tf.keras.preprocessing.text.Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ju-QItG5im2w"
   },
   "outputs": [],
   "source": [
    "t.fit_on_texts(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wlKG1BAim2y"
   },
   "source": [
    "Lets look into Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NkZudX6im2z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('ram', 1),\n",
       "             ('is', 2),\n",
       "             ('in', 2),\n",
       "             ('eighth', 1),\n",
       "             ('grade', 3),\n",
       "             ('and', 1),\n",
       "             ('ready', 1),\n",
       "             ('to', 2),\n",
       "             ('go', 1),\n",
       "             ('ninth', 1),\n",
       "             ('shanti', 1),\n",
       "             ('sixth', 1)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_counts  #Each word and how many times they appeared across all docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OlBApViFim22"
   },
   "source": [
    "How come its 'ram' and not 'Ram'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hPt7A3FBim23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.document_count #Number of documents Tokenizer got fit on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DUo0mim-im26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grade': 1,\n",
       " 'is': 2,\n",
       " 'in': 3,\n",
       " 'to': 4,\n",
       " 'ram': 5,\n",
       " 'eighth': 6,\n",
       " 'and': 7,\n",
       " 'ready': 8,\n",
       " 'go': 9,\n",
       " 'ninth': 10,\n",
       " 'shanti': 11,\n",
       " 'sixth': 12}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_index #Unique index of each word in Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vSGt-wZuim28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'is': 2,\n",
       "             'and': 1,\n",
       "             'ready': 1,\n",
       "             'eighth': 1,\n",
       "             'ram': 1,\n",
       "             'to': 1,\n",
       "             'go': 1,\n",
       "             'in': 2,\n",
       "             'ninth': 1,\n",
       "             'grade': 2,\n",
       "             'shanti': 1,\n",
       "             'sixth': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_docs #How many documents a word appeared in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KasrHSJJim2-"
   },
   "source": [
    "# Converting docs to Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j1Wo4Omwim2-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.texts_to_matrix(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysZD_xiwim3C"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 2., 1., 1., 2., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.texts_to_matrix(documents,mode='count')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2a. Exploring_Keras_Tokenizer_Class.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
