{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QK6Wtu8zR7W0"
   },
   "source": [
    "###Set the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKo_FGWlR7W1"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7gZ3PLlVR7XA"
   },
   "source": [
    "###Load data\n",
    "Data can be downloaded from Kaggle -> https://www.kaggle.com/c/word2vec-nlp-tutorial/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kxenUGDhR7W9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('labeledTrainData.tsv.zip',  #filepath\n",
    "                 header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMCZfxO8F4bb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LuOuT3-0R7XR"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JaF0c1HMR7XI"
   },
   "source": [
    "1.Split Data into Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VoYsGAgRR7XJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['review'],\n",
    "    df['sentiment'],\n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_VYq8asrSyiN"
   },
   "source": [
    "2.Build Tokenizer to get Number sequences for Each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqhDJm1FGMJB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\\\\"The Classic War of the Worlds\\\\\" by Timothy Hines is a very entertaining film that obviously goes to great effort and lengths to faithfully recreate H. G. Wells\\' classic book. Mr. Hines succeeds in doing so. I, and those who watched his film with me, appreciated the fact that it was not the standard, predictable Hollywood fare that comes out every year, e.g. the Spielberg version with Tom Cruise that had only the slightest resemblance to the book. Obviously, everyone looks for different things in a movie. Those who envision themselves as amateur \\\\\"critics\\\\\" look only to criticize everything they can. Others rate a movie on more important bases,like being entertained, which is why most people never agree with the \\\\\"critics\\\\\". We enjoyed the effort Mr. Hines put into being faithful to H.G. Wells\\' classic novel, and we found it to be very entertaining. This made it easy to overlook what the \\\\\"critics\\\\\" perceive to be its shortcomings.\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Y0jqE-hR7XS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Vocab size\n",
    "top_words = 10000\n",
    "\n",
    "t = Tokenizer(num_words=top_words)\n",
    "t.fit_on_texts(X_train.tolist())\n",
    "\n",
    "#Get the word index for each of the word in the review\n",
    "X_train = t.texts_to_sequences(X_train.tolist())\n",
    "X_test = t.texts_to_sequences(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cD5CrcmSupIT"
   },
   "outputs": [],
   "source": [
    "#X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jhsw0lTMnTo3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tbpwqn6iR7Xg"
   },
   "source": [
    "3.Pad sequences to make each review size equalGet the word index for each of the word in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D79z-4uoR7Xn"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "\n",
    "#Each review size\n",
    "max_review_length = 300\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train,maxlen=max_review_length,padding='post')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jtiWBlhPGv33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9NWBvhFUR7Xw"
   },
   "source": [
    "## Build Embedding Matrix from Pre-Trained Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pcLpvW5R7Xw"
   },
   "source": [
    "Load pre-trained Gensim Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rYebjVaXR7Xy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word2vec model..\n",
      "Model shape:  (28322, 50)\n"
     ]
    }
   ],
   "source": [
    "#Install gensim\n",
    "#!pip install gensim --quiet\n",
    "\n",
    "#Load pre-trained model\n",
    "import gensim\n",
    "word2vec = gensim.models.Word2Vec.load('word2vec-movie-50')\n",
    "\n",
    "#Embedding Length\n",
    "embedding_vector_length = word2vec.wv.vectors.shape[1]\n",
    "\n",
    "print('Loaded word2vec model..')\n",
    "print('Model shape: ', word2vec.wv.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FMswokZmocBF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28322, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2V5JPOl5R7X_"
   },
   "source": [
    "Build matrix for current data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OLL-LUZJR7YA"
   },
   "outputs": [],
   "source": [
    "#Initialize embedding matrix to all zeros\n",
    "embedding_matrix = np.zeros((top_words + 1, #Vocablury size + 1\n",
    "                             embedding_vector_length))\n",
    "\n",
    "#Steps for populating embedding matrix\n",
    "\n",
    "#1. Check each word in tokenizer vocablury to see if it exist in pre-trained\n",
    "# word2vec model.\n",
    "#2. If found, update embedding matrix with embeddings for the word \n",
    "# from word2vec model\n",
    "\n",
    "for word, i in sorted(t.word_index.items(),key=lambda x:x[1]):\n",
    "    if i > top_words:\n",
    "        break\n",
    "    if word in word2vec.wv.vocab:\n",
    "        embedding_vector = word2vec.wv[word]\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xdbd5_qXR7YE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.73459101, -0.34348151,  4.09425545, -0.76835114,  0.98946816,\n",
       "        1.87112844, -1.21309519,  0.02420728,  0.24738404, -3.80001116,\n",
       "        1.00591576, -0.00599149, -1.10720205, -0.64441431,  1.55634487,\n",
       "       -0.67932558,  2.68729401,  3.22928667,  1.98245931,  2.35130262,\n",
       "       -2.0172646 ,  2.68419147,  5.51142311, -1.83284128, -0.6304661 ,\n",
       "        1.92983949, -1.51213527, -2.33458519,  1.14439762,  0.23554215,\n",
       "       -3.18900323, -1.71647346, -2.8396821 ,  2.10814762,  1.59747708,\n",
       "       -2.05685472,  0.02134195, -1.49737895,  0.7752192 , -0.22689784,\n",
       "       -0.89800125, -1.75604141,  1.26355755, -2.64244699,  2.0314641 ,\n",
       "        1.48112845,  0.27211079, -0.79587841,  1.56275535, -4.55970097])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check embeddings for word 'great'\n",
    "embedding_matrix[t.word_index['great']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6vR1CQfR7YQ"
   },
   "source": [
    "## Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zAdor7TlqCN5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQ8mjqEBp7_K"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YDaaL9r5R7YR"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dropout, Dense, Embedding, Flatten\n",
    "\n",
    "#Build a sequential model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "086FRu-gR7Yc"
   },
   "source": [
    "Add Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SVOiv1D7R7Yd"
   },
   "outputs": [],
   "source": [
    "model.add(Embedding(top_words + 1, #Indexes that we need to deal with                    \n",
    "                    embedding_vector_length, #embedding_size i.e 50 in this case\n",
    "                    input_length=max_review_length, #Size of each review i.e 300 in this case\n",
    "                    weights=[embedding_matrix], #Pre-trained embedding\n",
    "                    trainable=False #We do not want to change embedding\n",
    "                   )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1trpI5Eo5MG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding/Identity:0' shape=(None, 300, 50) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vZzSadvdR7Yf"
   },
   "source": [
    "Output from Embedding is 3 dimension \n",
    "- batch_size x max_review_length x embedding_vector_length. \n",
    "\n",
    "We need to flatten the output for Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HR-kut27R7Yg"
   },
   "outputs": [],
   "source": [
    "#Flatten embedding layer output and flatten layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(200,activation='relu'))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(60,activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(30,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wtVKVBo5qJYW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 50)           500050    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 15000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200)               3000200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 60)                6060      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                1830      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 3,528,271\n",
      "Trainable params: 3,028,221\n",
      "Non-trainable params: 500,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SkielaGfR7Y9"
   },
   "source": [
    "## Execute the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtXiww1sR7Y9"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,\n",
    "          epochs=1,\n",
    "          batch_size=128,          \n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tth4i9AEXl0W"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nbBGh2DlR7ZA"
   },
   "outputs": [],
   "source": [
    "model.predict(X_test[100:102])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4. Sentiment_Pre_Trained_Embedding_FC.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
